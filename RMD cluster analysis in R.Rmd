---
title: "Cluster Analysis in R"
subtitle: "Chapters 1, 2, & 3"
author: "Sheldon Eng, credit to Dmitriy Gorenshtey and Datacamp"
date: "4/22/2022"
output: html_document
---

```{r setup, include = FALSE}
library(ggplot2)
library(dummies)
library(dplyr)
```

## Chapter 1

Calculating distance between observations

##### Exercise 1-1 When to cluster?

In which of these scenarios would clustering methods likely be appropriate?

1. Using consumer behavior data to identify distinct segments within a market.
2. Predicting whether a given user will click on an ad.
3. Identifying distinct groups of stocks that follow similar trading patterns.
4. Modeling & predicting GDP growth.

Answer: 1 & 3

###### Exercise 1-2 Calculate & plot the distance between two players

You've obtained the coordinates relative to the center of the field for two players in a soccer match and would like to calculate the distance between them.

In this exercise you will plot the positions of the 2 players and manually calculate the distance between them by using the Euclidean distance formula.

* Plot their positions from the `two_players` data frame using `ggplot`.
* Extract the positions of the players into two data frames `player1` and `player2.`
* Calculate the distance between player1 and player2 by using the Euclidean distance formula


```{r Exercise 1-2}
### pre-req data
two_players <- data.frame(x = c(5,15), y = c(4,10))

# Plot the positions of the players
ggplot(two_players, aes(x = x, y = y)) + 
  geom_point() +
  # Assuming a 40x60 field
  lims(x = c(-30,30), y = c(-20, 20))

# Split the players data frame into two observations
player1 <- two_players[1, ]
player2 <- two_players[2, ]

# Calculate and print their distance using the Euclidean Distance formula
player_distance <- sqrt( (player1$x - player2$x)^2 + (player1$y - player2$y)^2 )

player_distance

```

___

##### Exercise 1-3 Using the dist() function
Using the Euclidean formula manually may be practical for 2 observations but can get more complicated rather quickly when measuring the distance between many observations.

The `dist()` function simplifies this process by calculating distances between our observations (rows) using their features (columns). In this case the observations are the player positions and the dimensions are their x and y coordinates.

_Note: The default distance calculation for the dist() function is Euclidean distance_

* Calculate the distance between two players using the `dist()` function for the data frame `two_players`.
* Calculate the distance between three players for the data frame `three_players`.

```{r Exercise 1-3}
### pre-req data
three_players <- data.frame(x = c(5,15,0), y = c(4,10,20))

# Calculate the Distance Between two_players
dist_two_players <- dist(two_players)
dist_two_players

# Calculate the Distance Between three_players
dist_three_players <- dist(three_players)
dist_three_players

```

___

##### Exercise 1-4 Who are the closest players?

You are given the data frame containing the positions of 4 players on a soccer field.

This data is preloaded as `four_players` in your environment and is displayed below.

Work in the R console to answer the following question:

_Which two players are closest to one another?_

```{r Exercise 1-4}
#pre-req data
four_players <- data.frame(x = c(5,15,0,-5), y = c(4,10,20,5))

#
dist(four_players)

```

Answer: 1 & 4

___

##### Exercise 1-5 Effects of scale

You have learned that when a variable is on a larger scale than other variables in your data it may disproportionately influence the resulting distance calculated between your observations. Lets see this in action by observing a sample of data from the trees data set.

You will leverage the `scale()` function which by default centers & scales our column features.

Our variables are the following:

* Girth - tree diameter in inches
* Height - tree height in inches

```{r Exercise 1-5}
# pre-req data
three_trees <- data.frame(Girth = c(8.3,8.6,10.5), Height = c(840,780,864))

# Calculate distance for three_trees 
dist_trees <- dist(three_trees)

# Scale three trees & calculate the distance  
scaled_three_trees <- scale(three_trees)
dist_scaled_trees <- dist(scaled_three_trees)

# Output the results of both Matrices
print('Without Scaling')
dist_trees

print('With Scaling')
dist_scaled_trees

```

___

##### Exercise 1-6 When to scale data?

Below are examples of datasets and their corresponding features.

In which of these examples would scaling *not* be necessary?


1. Taxi Trips - tip earned ($), distance traveled (km).
2. Health Measurements of Individuals - height (meters), weight (grams), body fat percentage (%).
3. Student Attributes - average test score (1-100), distance from school (km), annual household income ($).
4. Salespeople Commissions - total yearly commision ($), number of trips taken.
5. None of the above, they all should be scaled when measuring distance.

Answer: 5 - None of the above, they all should be scaled when measuring distance.

___

##### Exercise 1-7 Calculating distance between categorical variables
In this exercise you will explore how to calculate binary (Jaccard) distances. In order to calculate distances we will first have to dummify our categories using the `dummy.data.frame()` from the library `dummies`.

You will use a small collection of survey observations stored in the data frame `job_survey` with the following columns:

* job_satisfaction: Possible options: "Hi", "Mid", "Low"
* is_happy: Possible options: "Yes", "No"

Instructions

* Create a dummified data frame `dummy_survey`.
* Generate a Jaccard distance matrix for the dummified survey data `dist_survey` using the `dist()` function using the parameter `method = 'binary'`.
* Print the original data and the distance matrix.
   * Note the observations with a distance of 0 in the original data (1, 2, and 4).


```{r Exercise 1-7}
# pre-req data
job_survey <- data.frame(job_satisfaction = c("Hi","Hi","Hi","Hi","Mid"), is_happy = c("No","No","No","Yes","No"))

# Dummify the Survey Data
dummy_survey <- dummy.data.frame(job_survey)

# Calculate the Distance
dist_survey <- dist(dummy_survey, method = "binary")

# Print the Original Data
job_survey

# Print the Distance Matrix
dist_survey

```

___

##### Exercise 1-8 The closest observation to a pair

Below you see a pre-calculated distance matrix between four players on a soccer field. You can clearly see that players 1 & 4 are the closest to one another with a Euclidean distance value of 10.

~ |  1  |  2  |  3  |
--|-----|-----|-----|
2 | 11.7|     |     |
3 | 16.8| 18.0|     |
4 | 10.0| 20.6| 15.8|

If 1 and 4 are the closest players among the four, which player is closest to players 1 and 4?


Answer: There isn't enough information to decide.

___


## Chapter 2

Hierarchical clustering


##### Exercise 2-1 Calculating linkage
Let us revisit the example with three players on a field. The distance matrix between these three players is shown below and is available as the variable `dist_players`.

From this we can tell that the first group that forms is between players 1 & 2, since they are the closest to one another with a Euclidean distance value of 11.

Now you want to apply the three linkage methods you have learned to determine what the distance of this group is to player 3.

~ |  1  |  2  |
--|-----|-----|
2 | 11  |     |
3 | 16  | 18  |


* Calculate the distance from player 3 to the group of players 1 & 2 using the following three linkage methods.
   * Complete: the resulting distance is based on the maximum.
   * Single: the resulting distance is based on the minimum.
   * Average: the resulting distance is based on the average.

```{r Exercise 2-1}
# pre-req data
dist_players <- dist_three_players

# Extract the pair distances
distance_1_2 <- dist_players[1]
distance_1_3 <- dist_players[2]
distance_2_3 <- dist_players[3]

# Calculate the complete distance between group 1-2 and 3
complete <- max(c(distance_1_3, distance_2_3))
complete

# Calculate the single distance between group 1-2 and 3
single <- min(c(distance_1_3, distance_2_3))
single

# Calculate the average distance between group 1-2 and 3
average <- mean(c(distance_1_3, distance_2_3))
average


```

___

##### Exercise 2-2 Revisited: The closest observation to a pair

You are now ready to answer this question!

Below you see a pre-calculated distance matrix between four players on a soccer field. You can clearly see that players 1 & 4 are the closest to one another with a Euclidean distance value of 10. This distance matrix is available for your exploration as the variable `dist_players`.

~ |  1  |  2  |  3  |
--|-----|-----|-----|
2 | 11.7|     |     |
3 | 16.8| 18.0|     |
4 | 10.0| 20.6| 15.8|


If 1 and 4 are the closest players among the four, which player is closest to players 1 and 4?



Answer: Complete Linkage: Player 3, Single & Average Linkage: Player 2
___

##### Exercise 2-3 Assign cluster membership
In this exercise you will leverage the `hclust()` function to calculate the iterative linkage steps and you will use the `cutree()` function to extract the cluster assignments for the desired number (k) of clusters.

You are given the positions of 12 players at the start of a 6v6 soccer match. This is stored in the `lineup` data frame.

You know that this match has two teams (k = 2), let's use the clustering methods you learned to assign which team each player belongs in based on their position.

Notes:

* The linkage method can be passed via the method parameter: 
   `hclust(distance_matrix, method = "complete")`
* Remember that in soccer opposing teams start on their half of the field.
* Because these positions are measured using the same scale we do not need to re-scale our data.


Instructions

* Calculate the Euclidean distance matrix `dist_players` among all twelve players.
* Perform the complete linkage calculation for hierarchical clustering using `hclust` and store this as `hc_players`.
* Build the cluster assignment vector `clusters_k2` using `cutree()` with a `k = 2`.
* Append the cluster assignments as a column `cluster` to the `lineup` data frame and save the results to a new data frame called `lineup_k2_complete`.

```{r Exercise 2-3}
# pre-req data ### alternatively, load from WD from this URL
lineup <- readRDS(url("https://assets.datacamp.com/production/repositories/1219/datasets/94af7037c5834527cc8799a9723ebf3b5af73015/lineup.rds"))

# Calculate the Distance
dist_players <- dist(lineup)

# Perform the hierarchical clustering using the complete linkage
hc_players <- hclust(dist_players, method = "complete")

# Calculate the assignment vector with a k of 2
clusters_k2 <- cutree(hc_players, k = 2)

# Create a new data frame storing these results
lineup_k2_complete <- mutate(lineup, cluster = clusters_k2)

```

___

##### Exercise 2-4 Exploring the clusters

Because clustering analysis is always in part qualitative, it is incredibly important to have the necessary tools to explore the results of the clustering.

In this exercise you will explore that data frame you created in the previous exercise `lineup_k2_complete`.

Reminder: The `lineup_k2_complete` data frame contains the x & y positions of 12 players at the start of a 6v6 soccer game to which you have added clustering assignments based on the following parameters:

* Distance: Euclidean
* Number of Clusters (k): 2
* Linkage Method: Complete

Instructions

* Using `count()` from dplyr, count the number of players assigned to each cluster.
* Using `ggplot()`, plot the positions of the players and color them by cluster assignment.

```{r Exercise 2-4}
# Count the cluster assignments
count(lineup_k2_complete, cluster)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_k2_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

```

___

##### Exercise 2-5 Validating the clusters

In the plot below you see the clustering results of the same lineup data you've previously worked with but with some minor modifications in the clustering steps.

* The left plot was generated using a `k=2` and `method = 'average'`
* The right plot was generated using a `k=3` and `method = 'complete'`


![](https://assets.datacamp.com/production/course_5592/datasets/c2_e7_example.png)

If our goal is to correctly assign each player to their correct team then based on what you see in the above plot and what you know about the data set which of the statements below are correct?


1. The left plot successfully clusters the players in their correct team.

2. The right plot successfully clusters the players in their correct team.

3. The left plot fails to correctly cluster the players;
because this is a 6v6 game the expectation is that both clusters should have 6 members each.

4. The right plot fails to correctly cluster the players;
because this is a two team match clustering into three unequal groups does not address the question correctly.

5. Answers 3 & 4 are both correct.


Answer: 5: Answers 3 & 4 are both correct.
___

##### Exercise 2-6 Comparing average, single & complete linkage

You are now ready to analyze the clustering results of the lineup dataset using the dendrogram plot. This will give you a new perspective on the effect the decision of the linkage method has on your resulting cluster analysis.


* Perform the linkage calculation for hierarchical clustering using the linkages: complete, single, and average.
* Plot the three dendrograms side by side and review the changes.

```{r Exercise 2-6}
# Prepare the Distance Matrix
dist_players <- dist(lineup)

# Generate hclust for complete, single & average linkage methods
hc_complete <- hclust(dist_players, method = "complete")
hc_single <- hclust(dist_players, method = "single")
hc_average <- hclust(dist_players, method = "average")

# Plot & Label the 3 Dendrograms Side-by-Side
# Hint: To see these Side-by-Side run the 4 lines together as one command
par(mfrow = c(1,3))
plot(hc_complete, main = 'Complete Linkage')
plot(hc_single, main = 'Single Linkage')
plot(hc_average, main = 'Average Linkage')


```

___

##### Exercise 2-7 Height of the tree

An advantage of working with a clustering method like hierarchical clustering is that you can describe the relationships between your observations based on both the distance metric and the linkage metric selected (the combination of which defines the height of the tree).

Based on the code below what can you concretely say about the height of a branch in the resulting dendrogram?

`dist_players <- dist(lineup, method = 'euclidean')`   
`hc_players <- hclust(dist_players, method = 'single')`   
`plot(hc_players)`  


All of the observations linked by this branch must have:


1. a maximum Euclidean distance amongst each other less than or equal to the height of the branch.
2. a minimum Jaccard distance amongst each other less than or equal to the height of the branch.
3. a minimum Euclidean distance amongst each other less than or equal to the height of the branch.


Answer: 3. a minimum Euclidean distance amongst each other less than or equal to the height of the branch.

___

##### Exercise 2-8 Clusters based on height

In previous exercises, you have grouped your observations into clusters using a pre-defined number of clusters (k). In this exercise, you will leverage the visual representation of the dendrogram in order to group your observations into clusters using a maximum height (h), below which clusters form.  

You will work the `color_branches()` function from the `dendextend` library in order to visually inspect the clusters that form at any height along the dendrogram.  

The hc_players has been carried over from your previous work with the soccer line-up data.  

* Create a dendrogram object `dend_players` from your `hclust` result using the function `as.dendrogram()`.
* Plot the dendrogram.
* Using the `color_branches()` function create & plot a new dendrogram with clusters colored by a cut height of 20.
* Repeat the above step with a height of 40.

```{r Exercise 2-8}
library(dendextend)

dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Create a dendrogram object from the hclust variable
dend_players <- as.dendrogram(hc_players)

# Plot the dendrogram
plot(dend_players)

# Color branches by cluster formed from the cut at a height of 20 & plot
dend_20 <- color_branches(dend_players, h = 20)

# Plot the dendrogram with clusters colored below height 20
plot(dend_20)

# Color branches by cluster formed from the cut at a height of 40 & plot
dend_40 <- color_branches(dend_players, h = 40)

# Plot the dendrogram with clusters colored below height 40
plot(dend_40)



```

___

##### Exercise 2-9 Exploring the branches cut from the tree

The `cutree()` function you used in exercises 5 & 6 can also be used to cut a tree at a given height by using the `h` parameter. Take a moment to explore the clusters you have generated from the previous exercises based on the heights 20 & 40.


* Build the cluster assignment vector `clusters_h20` using `cutree()` with a `h = 20`.
* Append the `cluster` assignments as a column cluster to the `lineup` data frame and save the results to a new data frame called `lineup_h20_complete`.
* Repeat the above two steps for a height of 40, generating the variables `clusters_h40` and `lineup_h40_complete`.
* Use ggplot2 to create a scatter plot, colored by the cluster assignment for both heights.

```{r Exercise 2-9}
dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = "complete")

# Calculate the assignment vector with a h of 20
clusters_h20 <- cutree(hc_players, h = 20)

# Create a new data frame storing these results
lineup_h20_complete <- mutate(lineup, cluster = clusters_h20)

# Calculate the assignment vector with a h of 40
clusters_h40 <- cutree(hc_players, h = 40)

# Create a new data frame storing these results
lineup_h40_complete <- mutate(lineup, cluster = clusters_h40)

# Plot the positions of the players and color them using their cluster for height = 20
ggplot(lineup_h20_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

# Plot the positions of the players and color them using their cluster for height = 40
ggplot(lineup_h40_complete, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()


```

___

##### Exercise 2-10 What do we know about our clusters?

Based on the code below, what can you concretely say about the relationships of the members within each cluster?

`dist_players <- dist(lineup, method = 'euclidean')`  
`hc_players <- hclust(dist_players, method = 'complete')`  
`clusters <- cutree(hc_players, h = 40)`  


Every member belonging to a cluster must have:


1. a maximum Euclidean distance to all other members of its cluster that is less than 40.
2. a maximum Euclidean distance to all other members of its cluster that is greater than or equal to 40.
3. a average Euclidean distance to all other members of its cluster that is less than 40.


Answer: 1. a maximum Euclidean distance to all other members of its cluster that is less than 40.

___

##### Exercise 2-11 Segment wholesale customers

You're now ready to use hierarchical clustering to perform market segmentation (i.e. use consumer characteristics to group them into subgroups).


In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the data frame `customers_spend`. Assign these clients into meaningful clusters.


Note: For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.



* Calculate the Euclidean distance between the customers and store this in `dist_customers`.
* Run hierarchical clustering using complete linkage and store in `hc_customers`.
* Plot the dendrogram.
* Create a cluster assignment vector using a height of 15,000 and store it as `clust_customers`.
* Generate a new data frame `segment_customers` by appending the cluster assignment as the column `cluster` to the original `customers_spend` data frame.


```{r Exercise 2-11}
# pre-req data ### alternatively, load from WD from this URL
customers_spend <- readRDS(url("https://assets.datacamp.com/production/repositories/1219/datasets/3558d2b5564714d85120cb77a904a2859bb3d03e/ws_customers.rds"))

# Calculate Euclidean distance between customers
dist_customers <- dist(customers_spend)

# Generate a complete linkage analysis 
hc_customers <- hclust(dist_customers, method = "complete")

# Plot the dendrogram
plot(hc_customers)

# Create a cluster assignment vector at h = 15000
clust_customers <- cutree(hc_customers, h = 15000)


# Generate the segmented customers data frame
segment_customers <- mutate(customers_spend, cluster = clust_customers)

```

___

##### Exercise 2-12 Explore wholesale customer clusters

Continuing your work on the wholesale dataset you are now ready to analyze the characteristics of these clusters.


Since you are working with more than 2 dimensions it would be challenging to visualize a scatter plot of the clusters, instead you will rely on summary statistics to explore these clusters. In this exercise you will analyze the mean amount spent in each cluster for all three categories.


* Calculate the size of each cluster using `count()`.
* Color & plot the dendrogram using the height of 15,000.
* Calculate the average spending for each category within each cluster using the `summarise_all()` function.

```{r Exercise 2-12}
dist_customers <- dist(customers_spend)
hc_customers <- hclust(dist_customers)
clust_customers <- cutree(hc_customers, h = 15000)
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Count the number of customers that fall into each cluster
count(segment_customers, cluster)

# Color the dendrogram based on the height cutoff
dend_customers <- as.dendrogram(hc_customers)
dend_colored <- color_branches(dend_customers, h = 15000)

# Plot the colored dendrogram
plot(dend_colored)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(list(mean))

```

___

##### Exercise 2-13 Interpreting the wholesale customer clusters

What observations can we make about our segments based on their average spending in each category?


|cluster| milk  | grocery| frozen| cluster size|
|-------|-------|--------|-------|-------------|
| 1     | 16950 |  12891 |  991  | 5           |
| 2     | 2512  |   5228 |  1795 | 29          |
| 3     | 10452 | 22550  |  1354 | 5           |
| 4     | 1249  | 3916   | 10888 | 6           |


  
1. Customers in cluster 1 spent more money on Milk than any other cluster.
2. Customers in cluster 3 spent more money on Grocery than any other cluster.
3. Customers in cluster 4 spent more money on Frozen goods than any other cluster.
4. The majority of customers fell into cluster 2 and did not show any excessive spending in any category.
5. All of the above.



Answer: 5. All of the above.
___

## Chapter 3

K-means clustering

##### Exercise 3-1 K-means on a soccer field

In the previous chapter, you used the `lineup` dataset to learn about hierarchical clustering, in this chapter you will use the same data to learn about k-means clustering. As a reminder, the `lineup` data frame contains the positions of 12 players at the start of a 6v6 soccer match.


Just like before, you know that this match has two teams on the field so you can perform a k-means analysis using k = 2 in order to determine which player belongs to which team.


Note that in the `kmeans()` function `k` is specified using the `centers` parameter.


* Build a k-means model called `model_km2` for the `lineup` data using the `kmeans()` function with `centers = 2`.
* Extract the vector of cluster assignments from the model `model_km2$cluster` and store this in the variable `clust_km2`.
* Append the cluster assignments as a column `cluster` to the `lineup` data frame and save the results to a new data frame called `lineup_km2`.
* Use ggplot to plot the positions of each player on the field and color them by their cluster.

```{r Exercise 3-1}
# Build a kmeans model
model_km2 <- kmeans(lineup, centers = 2)

# Extract the cluster assignment vector from the kmeans model
clust_km2 <- model_km2$cluster

# Create a new data frame appending the cluster assignment
lineup_km2 <- mutate(lineup, cluster = clust_km2)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km2, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

```

___

##### Exercise 3-2 K-means on a soccer field (part 2)

In the previous exercise, you successfully used the k-means algorithm to cluster the two teams from the `lineup` data frame. This time, let's explore what happens when you use a `k` of 3.


You will see that the algorithm will still run, but does it actually make sense in this context…


* Build a k-means model called `model_km3` for the `lineup` data using the `kmeans()` function with `centers = 3`.
* Extract the vector of cluster assignments from the model `model_km3$cluster` and store this in the variable `clust_km3`.
* Append the cluster assignments as a column `cluster` to the `lineup` data frame and save the results to a new data frame called `lineup_km3`.
* Use ggplot to plot the positions of each player on the field and color them by their cluster.

```{r Exercise 3-2}
# Build a kmeans model
model_km3 <- kmeans(lineup, centers = 3)

# Extract the cluster assignment vector from the kmeans model
clust_km3 <- model_km3$cluster

# Create a new data frame appending the cluster assignment
lineup_km3 <- mutate(lineup, cluster = clust_km3)

# Plot the positions of the players and color them using their cluster
ggplot(lineup_km3, aes(x = x, y = y, color = factor(cluster))) +
  geom_point()

```

___

##### Exercise 3-3 Many K's many models

While the `lineup` dataset clearly has a known value of k, often times the optimal number of clusters isn't known and must be estimated.


In this exercise you will leverage `map_dbl()` from the `purrr` library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares metric from each one. This will be the first step towards visualizing the elbow plot.


* Use `map_dbl()` to run `kmeans()` using the `lineup` data for k values ranging from 1 to 10 and extract the total within-cluster sum of squares value from each model:` model$tot.withinss`. Store the resulting vector as `tot_withinss`.
* Build a new data frame `elbow_df` containing the values of k and the vector of total within-cluster sum of squares.


```{r Exercise 3-3}
library(purrr)

# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)

```

___

##### Exercise 3-4 Elbow (Scree) plot

In the previous exercises you have calculated the total within-cluster sum of squares for values of k ranging from 1 to 10. You can visualize this relationship using a line plot to create what is known as an elbow plot (or scree plot).


When looking at an elbow plot you want to see a sharp decline from one k to another followed by a more gradual decrease in slope. The last value of k before the slope of the plot levels off suggests a "good" value of k.


* Continuing your work from the previous exercise, use the values in `elbow_df` to plot a line plot showing the relationship between k and total within-cluster sum of squares.

```{r Exercise 3-4}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10, function(k){
  model <- kmeans(x = lineup, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)

```

___

##### Exercise 3-5 Interpreting the elbow plot

Based on the elbow plot you generated in the previous exercise for the lineup data:



Which of these interpretations are valid?


1. Based on this plot, the k to choose is 2; the elbow occurs there.
2. The k to choose is 5; this is where the trend levels off.
3. Any value of k is valid; this plot does not clearly identify an elbow.
4. None of the above.


Answer: 1. Based on this plot, the k to choose is 2; the elbow occurs there.

___

##### Exercise 3-6  Silhouette analysis

Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

* Values close to 1 suggest that the observation is well matched to the assigned cluster
* Values close to 0 suggest that the observation is borderline matched between two clusters
* Values close to -1 suggest that the observations may be assigned to the wrong cluster

In this exercise you will leverage the `pam()` and the `silhouette()` functions from the `cluster` library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You'll continue working with the `lineup` dataset.


_Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k = 3?_


* Generate a k-means model `pam_k2` using `pam()` with `k = 2` on the `lineup` data.
* Plot the silhouette analysis using `plot(silhouette(model))`.
* Repeat the first two steps for `k = 3`, saving the model as `pam_k3`.
* Make sure to review the differences between the plots before proceeding (especially observation 3) for `pam_k3`.


```{r Exercise 3-6}
library(cluster)

# Generate a k-means model using the pam() function with a k = 2
pam_k2 <- pam(lineup, k = 2)

# Plot the silhouette visual for the pam_k2 model
plot(silhouette(pam_k2))

# Generate a k-means model using the pam() function with a k = 3
pam_k3 <- pam(lineup, k = 3)

# Plot the silhouette visual for the pam_k3 model
plot(silhouette(pam_k3))

```
___

##### Exercise 3-7 Revisiting wholesale data: "Best" k

At the end of _Chapter 2_ you explored wholesale distributor data `customers_spend` using hierarchical clustering. This time you will analyze this data using the k-means clustering tools covered in this chapter.  


The first step will be to determine the "best" value of k using average silhouette width.


A refresher about the data: it contains records of the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the data frame `customers_spend`. For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.

 

* Use `map_dbl()` to run `pam()` using the `customers_spend` data for k values ranging from 2 to 10 and extract the average silhouette width value from each model: `model$silinfo$avg.width`. Store the resulting vector as `sil_width`.
* Build a new data frame `sil_df` containing the values of k and the vector of average silhouette widths.
* Use the values in `sil_df` to plot a line plot showing the relationship between k and average silhouette width.

```{r Exercise 3-7}
# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = customers_spend, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)

```

___

##### Exercise 3-8 Revisiting wholesale data: Exploration

From the previous analysis you have found that `k = 2` has the highest average silhouette width. In this exercise you will continue to analyze the wholesale customer data by building and exploring a kmeans model with 2 clusters.



* Build a k-means model called `model_customers` for the `customers_spend` data using the `kmeans()` function with `centers = 2`.
* Extract the vector of cluster assignments from the model `model_customers$cluster` and store this in the variable `clust_customers`.
* Append the cluster assignments as a column `cluster` to the `customers_spend` data frame and save the results to a new data frame called `segment_customers`.
* Calculate the size of each cluster using `count()`.

```{r Exercise 3-8}
set.seed(42)

# Build a k-means model for the customers_spend with a k of 2
model_customers <- kmeans(customers_spend, centers = 2)

# Extract the vector of cluster assignments from the model
clust_customers <- model_customers$cluster

# Build the segment_customers data frame
segment_customers <- mutate(customers_spend, cluster = clust_customers)

# Calculate the size of each cluster
count(segment_customers, cluster)

# Calculate the mean for each category
segment_customers %>% 
  group_by(cluster) %>% 
  summarise_all(list(mean))

```

___
___